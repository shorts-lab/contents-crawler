from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import urllib.parse
import json
import time

def get_search_url(keyword, page):
    base_url = "https://section.blog.naver.com/Search/Post.naver"
    encoded_keyword = urllib.parse.quote(keyword)
    return f"{base_url}?pageNo={page}&rangeType=ALL&orderBy=recentdate&keyword={encoded_keyword}"

def extract_blog_content(driver, url):
    try:
        driver.get(url)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # ë¸”ë¡œê·¸ iframe ì•ˆì— ì‹¤ì œ ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš° ëŒ€ì‘
        frame = driver.find_elements(By.CSS_SELECTOR, "iframe#mainFrame")
        if frame:
            driver.switch_to.frame(frame[0])
            time.sleep(2)
            soup = BeautifulSoup(driver.page_source, "html.parser")

        # ë³¸ë¬¸ ì¶”ì¶œ: Naver ë¸”ë¡œê·¸ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤ ëŒ€ì‘
        content_div = soup.select_one("div.se-main-container") or \
                      soup.select_one("div#postViewArea") or \
                      soup.select_one("div#contentArea")

        if content_div:
            return content_div.get_text(separator="\n").strip()
        else:
            return "(ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤)"
    except Exception as e:
        return f"(ì˜¤ë¥˜ ë°œìƒ: {e})"

def crawl_naver_blog(keyword, max_pages=1):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(options=options)

    results = []

    try:
        for page in range(1, max_pages + 1):
            url = get_search_url(keyword, page)
            print(f"ğŸ” Crawling page {page} ...")
            driver.get(url)
            time.sleep(3)

            soup = BeautifulSoup(driver.page_source, "html.parser")
            blog_cards = soup.select("div.list_search_post > div")

            for card in blog_cards:
                link_tag = card.select_one("a.desc_inner")
                if link_tag:
                    link = link_tag["href"]
                    title = link_tag.text.strip()
                    print(f"  â†’ ë¸”ë¡œê·¸ ìˆ˜ì§‘ ì¤‘: {title}")
                    content = extract_blog_content(driver, link)
                    results.append({
                        "title": title,
                        "url": link,
                        "content": content
                    })
    finally:
        driver.quit()

    return results

if __name__ == "__main__":
    keyword = input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì—°ì• ): ").strip()
    pages = int(input("í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 3): "))
    data = crawl_naver_blog(keyword, pages)

    filename = f"naver_blog_{keyword}.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… {len(data)}ê°œì˜ ë¸”ë¡œê·¸ ê¸€ì´ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
